# Abdominal Medical Image Segmentation with 2D U-Net

This is a README file generated by Claude Sonnet 4.5

A deep learning project for multi-organ segmentation in abdominal CT scans using a 2D U-Net architecture with weighted loss functions.

![](poster.png)

## ğŸ“‹ Overview

This project implements an end-to-end pipeline for segmenting 12 organ classes in abdominal CT scans, including:
- **Background** (0)
- **Bladder** (1), **Colon** (2), **Femur Head** (3), **Kidney** (4), **Liver** (5)
- **Rectum** (6), **Small Intestine** (7), **Spinal Cord** (8), **Stomach** (9)
- **Spleen** (10), **Pancreas** (11)

The model achieves robust segmentation performance by combining:
- 2D U-Net architecture with batch normalization
- Weighted Cross-Entropy loss with distance-based boundary weighting
- Data augmentation (rotations, brightness/contrast variations)
- Mixed precision training with gradient accumulation

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ network/
â”‚   â””â”€â”€ Unet.py                    # U-Net model architecture
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset.py                 # PyTorch Dataset classes and data loaders
â”‚   â”œâ”€â”€ loss_and_metrics.py        # Loss functions (Weighted CE) and metrics (Dice, IoU)
â”‚   â”œâ”€â”€ preprocess_data.py         # Data preprocessing pipeline
â”‚   â””â”€â”€ dataset_introduction.ipynb # Dataset visualization and exploration
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ med_segmentation.ipynb     # Main training notebook
â”‚   â””â”€â”€ visualization.py           # Prediction visualization utilities
â”œâ”€â”€ best_unet_model.pth            # Trained model weights
â””â”€â”€ README.md                       # This file
```

## ğŸ“Š Datasets

This project uses two complementary CT datasets:

### 1. PKU Dataset
- Private dataset with 11 organ classes
- Image size: 512Ã—512Ã—N (variable depth)
- Original labels: Bladder, Colon, Femur Heads (L/R), Kidneys (L/R), Liver, Rectum, Small Intestine, Spinal Cord, Stomach

### 2. AbdomenCT-1K Dataset
- Publicly available dataset
- 41 paired CT scans and masks (filtered from full dataset)
- Image size: 512Ã—512Ã—N
- Original labels: Liver, Kidney, Spleen, Pancreas

**Label Harmonization:** Both datasets are remapped to a unified 12-class label scheme to enable combined training.

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install torch torchvision nibabel numpy scipy scikit-learn matplotlib ipywidgets wandb
```

### Data Preprocessing

1. **Explore the datasets** (optional):
   ```bash
   jupyter notebook utils/dataset_introduction.ipynb
   ```
   This notebook provides visualization of CT slices, masks, HU value distributions, and label frequencies.

2. **Preprocess 3D volumes into 2D slices**:
   
   The preprocessing pipeline:
   - Loads 3D NIfTI (.nii.gz) files
   - Resizes to 256Ã—256 pixels
   - Clips HU values to [-400, 400] and normalizes
   - Computes distance maps for boundary-weighted loss
   - Generates augmented variations (4 brightness variations Ã— 4 rotations)
   - Saves as .npy files for efficient loading

   Run the preprocessing cells in [training/med_segmentation.ipynb](training/med_segmentation.ipynb):
   ```python
   from utils.preprocess_data import preprocess_pku_dataset, preprocess_other_dataset
   
   # Process PKU dataset
   preprocess_pku_dataset(
       image_input_path='path/to/pku/ct/',
       mask_input_path='path/to/pku/label/',
       output_base_path='path/to/output/',
       min_clip_value=-400,
       max_clip_value=400,
       target_size=(256, 256),
       num_brightness_variations=4
   )
   
   # Process AbdomenCT-1K dataset
   preprocess_other_dataset(
       image_input_path='path/to/abdomen/Image/',
       mask_input_path='path/to/abdomen/Mask/',
       output_base_path='path/to/output/',
       min_clip_value=-400,
       max_clip_value=400,
       target_size=(256, 256),
       num_brightness_variations=4
   )
   ```

### Training

Open and run [training/med_segmentation.ipynb](training/med_segmentation.ipynb):

1. **Load preprocessed data**:
   ```python
   from utils.dataset import load_and_pair_files, create_train_val_datasets
   
   paired_file_paths = load_and_pair_files('path/to/processed_data/')
   train_dataset, val_dataset = create_train_val_datasets(paired_file_paths, test_size=0.2)
   ```

2. **Initialize model and training**:
   ```python
   from network.Unet import UNet
   from utils.loss_and_metrics import WeightedCELoss
   
   model = UNet(in_channels=1, out_channels=12, features=[64, 128, 256, 512])
   criterion = WeightedCELoss(num_classes=12, class_weights=weights, sigma=3)
   ```

3. **Train with Weights & Biases logging**:
   - The notebook includes full training loop with:
     - Mixed precision training (AMP)
     - Gradient accumulation
     - StepLR scheduler
     - Per-class Dice and IoU tracking
     - Model checkpointing

### Inference and Visualization

Visualize predictions on full 3D volumes:

```python
from visualization import (load_and_preprocess_ct_data, generate_predictions,
                          create_interactive_visualization)

# Load and preprocess CT volume
ct_data, mask_data = load_and_preprocess_ct_data(
    ct_image_path='path/to/ct.nii.gz',
    mask_path='path/to/mask.nii.gz',
    target_size=(256, 256),
    min_clip_value=-400,
    max_clip_value=400,
    label_rearrange=pku_label_rearrange
)

# Generate predictions
predicted_masks = generate_predictions(model, ct_data, device)

# Interactive visualization with slider
create_interactive_visualization(ct_data, mask_data, predicted_masks, num_classes=12)
```

## ğŸ—ï¸ Model Architecture

### U-Net
- **Encoder**: 4 downsampling blocks with max pooling
- **Bottleneck**: 1024 features
- **Decoder**: 4 upsampling blocks with skip connections
- **Features**: [64, 128, 256, 512]
- **Activation**: ReLU
- **Normalization**: Batch Normalization
- **Output**: 12-channel segmentation map (one per class)

### Loss Function

**Weighted Cross-Entropy Loss** with two components:

1. **Standard Weighted CE** (`loss_ce1`): Class-balanced cross-entropy
2. **Distance-Weighted CE** (`loss_ce2`): Emphasizes pixels near organ boundaries using Gaussian weighting:
   
   $$w(x) = \exp\left(-\frac{d(x)^2}{2\sigma^2}\right)$$
   
   where $d(x)$ is the distance to the nearest boundary.

Total loss: `loss = loss_ce1 + Î± Ã— loss_ce2` (Î±=10, Ïƒ=3)

## ğŸ“ˆ Evaluation Metrics

- **Dice Score**: $\frac{2 \times |X \cap Y|}{|X| + |Y|}$
- **IoU (Intersection over Union)**: $\frac{|X \cap Y|}{|X \cup Y|}$

Both metrics are computed:
- Per-class (for each of 11 organs, excluding background)
- Only on slices where the organ appears in ground truth
- Averaged across the validation set

## ğŸ”§ Key Features

1. **Data Augmentation**:
   - 4 rotation angles (0Â°, 90Â°, 180Â°, 270Â°)
   - 4 brightness/contrast variations
   - 16Ã— data augmentation per slice

2. **Memory Optimization**:
   - Mixed precision training (FP16)
   - Gradient accumulation
   - Efficient .npy file format
   - Optional gradient checkpointing

3. **Training Monitoring**:
   - Weights & Biases integration
   - Per-class metrics logging
   - Learning rate scheduling
   - Best model checkpointing

## ğŸ“ Usage Notes

- **HU Windowing**: Default [-400, 400] focuses on soft tissue. Adjust for different tissue types.
- **Image Size**: Resized to 256Ã—256 for computational efficiency. Can be adjusted in preprocessing.
- **Class Weights**: Manually tuned to `[0.3, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0]` based on label frequency and importance.

## ğŸ¯ Results

The trained model (`best_unet_model.pth`) achieves strong segmentation performance across all organ classes. Use the visualization notebook to:
- View slice-by-slice predictions with interactive slider
- Compare predictions against ground truth
- Analyze per-organ Dice scores with bar charts

However, the model faces challenges in generality due to missing annotations in the training data. 
In other words, the model is not "bold" enough to predict the existence of organs on pixels which are labeled as background in the training dataset but are in fact organs.

In fact, our model is less affected by the missing annotation issue when it inferences on test datasets, compared to working on training and validation dataset.

## ğŸ“š References

- U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)
- AbdomenCT-1K Dataset
- PKU Medical Image Segmentation Dataset

## ğŸ‘¤ Author

Final project for Introduction to Deep Learning course at Rice University.

## ğŸ“„ License

This project is for academic purposes.
