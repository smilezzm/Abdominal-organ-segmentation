{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRKdHLrEU6rt",
        "outputId": "0ba09c11-7802-44bb-ce84-77f21ce78ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "\n",
        "# !pip install nibabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37LrXu90fpWr"
      },
      "source": [
        "It's helpful to first build an intuition on the dataset. We already know the pku data are of size (512,512,N), and there may be missing organ indices on some slices. Let's see the size and labels of the \"other\" data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "63cfcc49"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import nibabel as nib\n",
        "# import numpy as np\n",
        "\n",
        "# # folder_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Mask/'\n",
        "# folder_path = '/content/drive/MyDrive/med_segmentation/pku_train_dataset/label/'\n",
        "\n",
        "# unique_integers_list = []\n",
        "# file_sizes_list = []\n",
        "\n",
        "# # Check if path exists first\n",
        "# if os.path.exists(folder_path):\n",
        "#     # Get all .nii.gz files sorted to ensure consistent order\n",
        "#     nii_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.nii.gz')])\n",
        "\n",
        "#     print(f\"Found {len(nii_files)} files. Processing...\")\n",
        "\n",
        "#     for f in nii_files:\n",
        "#         file_path = os.path.join(folder_path, f)\n",
        "#         try:\n",
        "#             img = nib.load(file_path)\n",
        "#             # get_fdata() loads data as float, casting to int since you mentioned pixels are integers\n",
        "#             data = img.get_fdata().astype(int)\n",
        "\n",
        "#             # Get unique values\n",
        "#             unique_vals = np.unique(data).tolist()\n",
        "#             unique_integers_list.append(unique_vals)\n",
        "\n",
        "#             # Get shape (Length, Width, Height)\n",
        "#             file_sizes_list.append(list(img.shape))\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing {f}: {e}\")\n",
        "\n",
        "#     print(\"Unique integers list:\")\n",
        "#     print(unique_integers_list)\n",
        "#     print(\"\\nFile sizes list:\")\n",
        "#     print(file_sizes_list)\n",
        "# else:\n",
        "#     print(f\"Directory not found: {folder_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0cb4zslb4s2"
      },
      "source": [
        "It seems that the pku data and other sources of data are both of size (512,512,x). But not all indices are appearing on a certain slice.\n",
        "\n",
        "Next, let's see what's contained in the header (matadata) of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "248425fe"
      },
      "outputs": [],
      "source": [
        "# folder_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Image/'\n",
        "# file_name = 'Case_00002_0000.nii.gz'\n",
        "# file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "# if os.path.exists(file_path):\n",
        "#     try:\n",
        "#         img = nib.load(file_path)\n",
        "#         print(f\"Metadata for {file_name}:\")\n",
        "#         # The header contains various metadata about the NIfTI file\n",
        "#         print(img.header)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading or processing {file_name}: {e}\")\n",
        "# else:\n",
        "#     print(f\"File not found: {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMyBWfdumF-k"
      },
      "source": [
        "Among these various matadata, qoffset is just a translation, setting the position of the origin (i=0,j=0,k=0) in real space.\n",
        "\n",
        "One thing to notice is the physical size for each pixel of these .nii.gz CT image, which is stored in pixdim[1:4]. For example, some may have $\\Delta z=4mm$ but the others may have $\\Delta z=1mm$. This means that one pixel corresponds to different real-space physical volume. However, it seems that we don't need to care about $\\Delta z$, since we only consider 2D images. Different $\\Delta x,\\Delta y$ also don't matter, since it in fact acts as scaling, saving the step of performing this data augmentation process.\n",
        "\n",
        "Below shows the histogram of physical pixel sizes among the 'other' dataset and pku dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bHvNqLZdkZSL"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# # for other dataset\n",
        "# folder_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Image/'\n",
        "\n",
        "# x_spacings = []\n",
        "# y_spacings = []\n",
        "# z_spacings = []\n",
        "\n",
        "# if os.path.exists(folder_path):\n",
        "#     nii_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.nii.gz')])\n",
        "#     print(f\"Found {len(nii_files)} files. collecting metadata...\")\n",
        "\n",
        "#     for f in nii_files:\n",
        "#         file_path = os.path.join(folder_path, f)\n",
        "#         try:\n",
        "#             img = nib.load(file_path)\n",
        "#             header = img.header\n",
        "#             # pixdim[1], [2], [3] correspond to spatial dimensions x, y, z\n",
        "#             x_spacings.append(header['pixdim'][1])\n",
        "#             y_spacings.append(header['pixdim'][2])\n",
        "#             z_spacings.append(header['pixdim'][3])\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error reading {f}: {e}\")\n",
        "\n",
        "#     # Plotting\n",
        "#     fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "#     axes[0].hist(x_spacings, bins=20, color='skyblue', edgecolor='black')\n",
        "#     axes[0].set_title('Histogram of Pixel Size (Dim 1) in other dataset')\n",
        "#     axes[0].set_xlabel('Size (mm)')\n",
        "#     axes[0].set_ylabel('Count')\n",
        "\n",
        "#     axes[1].hist(y_spacings, bins=20, color='lightgreen', edgecolor='black')\n",
        "#     axes[1].set_title('Histogram of Pixel Size (Dim 2) in other dataset')\n",
        "#     axes[1].set_xlabel('Size (mm)')\n",
        "\n",
        "#     axes[2].hist(z_spacings, bins=20, color='salmon', edgecolor='black')\n",
        "#     axes[2].set_title('Histogram of Pixel Size (Dim 3) in other dataset')\n",
        "#     axes[2].set_xlabel('Size (mm)')\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# else:\n",
        "#     print(f\"Directory not found: {folder_path}\")\n",
        "\n",
        "# # For pku dataset\n",
        "# folder_path = '/content/drive/MyDrive/med_segmentation/pku_train_dataset/ct/'\n",
        "\n",
        "# x_spacings = []\n",
        "# y_spacings = []\n",
        "# z_spacings = []\n",
        "\n",
        "# if os.path.exists(folder_path):\n",
        "#     nii_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.nii.gz')])\n",
        "#     print(f\"Found {len(nii_files)} files. collecting metadata...\")\n",
        "\n",
        "#     for f in nii_files:\n",
        "#         file_path = os.path.join(folder_path, f)\n",
        "#         try:\n",
        "#             img = nib.load(file_path)\n",
        "#             header = img.header\n",
        "#             # pixdim[1], [2], [3] correspond to spatial dimensions x, y, z\n",
        "#             x_spacings.append(header['pixdim'][1])\n",
        "#             y_spacings.append(header['pixdim'][2])\n",
        "#             z_spacings.append(header['pixdim'][3])\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error reading {f}: {e}\")\n",
        "\n",
        "#     # Plotting\n",
        "#     fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "#     axes[0].hist(x_spacings, bins=20, color='skyblue', edgecolor='black')\n",
        "#     axes[0].set_title('Histogram of Pixel Size (Dim 1) in pku dataset')\n",
        "#     axes[0].set_xlabel('Size (mm)')\n",
        "#     axes[0].set_ylabel('Count')\n",
        "\n",
        "#     axes[1].hist(y_spacings, bins=20, color='lightgreen', edgecolor='black')\n",
        "#     axes[1].set_title('Histogram of Pixel Size (Dim 2) in pku dataset')\n",
        "#     axes[1].set_xlabel('Size (mm)')\n",
        "\n",
        "#     axes[2].hist(z_spacings, bins=20, color='salmon', edgecolor='black')\n",
        "#     axes[2].set_title('Histogram of Pixel Size (Dim 3) in pku dataset')\n",
        "#     axes[2].set_xlabel('Size (mm)')\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# else:\n",
        "#     print(f\"Directory not found: {folder_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q4LiU5glxfK"
      },
      "source": [
        "Also, for 'other' dataset, not every CT .nii.gz image has corresponding label file. So I'll remove those without labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVqlBr6KmX7P",
        "outputId": "79284fa9-b639-4f7a-b2fc-6320259723e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 41 mask files. Extracting identifiers...\n",
            "41 files remain in Image directory: /content/drive/MyDrive/med_segmentation/other_train_dataset/Image/\n",
            "0 files in Mask have no corresponding images:  set()\n"
          ]
        }
      ],
      "source": [
        "Image_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Image/'\n",
        "Mask_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Mask/'\n",
        "mask_idx = set()\n",
        "\n",
        "if os.path.exists(Mask_path):\n",
        "  nii_files = sorted([f for f in os.listdir(Mask_path) if f.endswith('.nii.gz')])\n",
        "  # f looks like Case_00001.nii.gz\n",
        "  for f in nii_files:\n",
        "    identifier = f.replace('Case_', '').replace('.nii.gz', '')\n",
        "    mask_idx.add(identifier)\n",
        "\n",
        "  print(f\"Found {len(nii_files)} mask files. Extracting identifiers...\")\n",
        "\n",
        "else:\n",
        "  print(f\"Mask directory not found: {Mask_path}\")\n",
        "\n",
        "\n",
        "if os.path.exists(Image_path):\n",
        "  nii_files = sorted([f for f in os.listdir(Image_path) if f.endswith('.nii.gz')])\n",
        "  num = len(nii_files)\n",
        "  for f in nii_files:\n",
        "    identifier = f.replace('Case_', '').replace('_0000.nii.gz', '')\n",
        "\n",
        "    if identifier not in mask_idx:\n",
        "      print(f\"Removing {f}...\")\n",
        "      os.remove(os.path.join(Image_path, f))\n",
        "      num -= 1\n",
        "    else:\n",
        "      mask_idx.remove(identifier)\n",
        "  print(f\"{num} files remain in Image directory: {Image_path}\")\n",
        "  print(f\"{len(mask_idx)} files in Mask have no corresponding images:\\\n",
        "  {mask_idx}\")\n",
        "else:\n",
        "  print(f\"Image directory not found: {Image_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8XyzvxATSnD"
      },
      "source": [
        "Let's then build some intuition on the CT image, especially the range of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "51d7009a"
      },
      "outputs": [],
      "source": [
        "# # 1. Define the folder path\n",
        "# folder_path = '/content/drive/MyDrive/med_segmentation/pku_train_dataset/ct/'\n",
        "\n",
        "# # 2. Specify the file name\n",
        "# file_name = '1.nii.gz'\n",
        "\n",
        "# # 3. Construct the full file path\n",
        "# file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "# # 4. Load the .nii.gz image file\n",
        "# if os.path.exists(file_path):\n",
        "#     img = nib.load(file_path)\n",
        "#     print(f\"Successfully loaded {file_name}.\")\n",
        "#     print(f\"Image data shape: {img.shape}\")\n",
        "# else:\n",
        "#     print(f\"Error: File not found at {file_path}\")\n",
        "\n",
        "# data = img.get_fdata()\n",
        "# print(f\"Data type: {data.dtype}\")\n",
        "# print(f\"Data shape: {data.shape}\")\n",
        "# print(f\"Minimum pixel value: {data.min()}\")\n",
        "# print(f\"Maximum pixel value: {data.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dfa80fa8"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from ipywidgets import interact, IntSlider, fixed\n",
        "\n",
        "# def plot_slice(slice_idx, data, global_min, global_max):\n",
        "#     plt.figure(figsize=(10, 10))\n",
        "#     plt.imshow(data[:, :, slice_idx], cmap='gray', vmin=global_min, vmax=global_max)\n",
        "#     plt.title(f'Slice {slice_idx + 1}/{data.shape[2]}')\n",
        "#     plt.axis('off')\n",
        "#     cbar = plt.colorbar(shrink=0.7)\n",
        "#     cbar.set_label('Pixel Value (HU)')\n",
        "#     plt.show()\n",
        "\n",
        "# # Get global min and max for consistent color scaling across all slices\n",
        "# global_min_value = data.min()\n",
        "# global_max_value = data.max()\n",
        "\n",
        "# # Create an interactive slider for scrolling through slices\n",
        "# interact(plot_slice,\n",
        "#          slice_idx=IntSlider(min=0, max=data.shape[2]-1, step=1, value=data.shape[2]//2,\n",
        "#                              description='Slice Index'),\n",
        "#          data=fixed(data),\n",
        "#          global_min=fixed(global_min_value),\n",
        "#          global_max=fixed(global_max_value)\n",
        "#         );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UisClQ30Kc-B"
      },
      "source": [
        "Next, I'll preprocess the .nii.gz files to generate numerous .npy files, each corresponding 2D data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbVXKaHINKg8"
      },
      "source": [
        "Notice: In 'other' dataset, the labels are defined as:\n",
        "\n",
        "liver (label=1), kidney (label=2), spleen (label=3), and pancreas (label=4)\n",
        "\n",
        "But in pku dataset, the labels are defined as:\n",
        "\n",
        "1: Bladder, 2: Colon, 3: Femur Head L, 4: Femur Head R, 5: Kidney L, 6: Kidney R, 7: Liver, 8: Rectum, 9: SmallIntestine, 10: SpinalCord, 11: Stomach\n",
        "\n",
        "To simultaneously take advantage of both datasets, in the following process I'll rearrange the label files defined as:\n",
        "\n",
        "0: Background, 1: Bladder, 2: Colon, 3: Femur Head, 4: Kidney, 5: Liver, 6: Rectum, 7: SmallIntestine, 8: SpinalCord, 9: Stomach, 10: Spleen, 11: Pancreas\n",
        "\n",
        "Moreover, in order to implement weighted dice loss for training process, we need to record the frequency of each index (except for the slices without annotation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e807f63c",
        "outputId": "4f41d21e-99c1-44ab-f191-13c6995d1eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting processing of 72 image-mask pairs...\n",
            "Processed 1/72: 1 (76 slices, 4 variations each)\n",
            "Processed 2/72: 100 (93 slices, 4 variations each)\n",
            "Processed 3/72: 102 (107 slices, 4 variations each)\n",
            "Processed 4/72: 103 (111 slices, 4 variations each)\n",
            "Processed 5/72: 105 (92 slices, 4 variations each)\n",
            "Processed 6/72: 106 (107 slices, 4 variations each)\n",
            "Processed 7/72: 107 (98 slices, 4 variations each)\n",
            "Processed 8/72: 11 (74 slices, 4 variations each)\n",
            "Processed 9/72: 12 (89 slices, 4 variations each)\n",
            "Processed 10/72: 13 (81 slices, 4 variations each)\n",
            "Processed 11/72: 14 (87 slices, 4 variations each)\n",
            "Processed 12/72: 16 (80 slices, 4 variations each)\n",
            "Processed 13/72: 17 (83 slices, 4 variations each)\n",
            "Processed 14/72: 2 (89 slices, 4 variations each)\n",
            "Processed 15/72: 20 (76 slices, 4 variations each)\n",
            "Processed 16/72: 21 (72 slices, 4 variations each)\n",
            "Processed 17/72: 22 (69 slices, 4 variations each)\n",
            "Processed 18/72: 23 (69 slices, 4 variations each)\n",
            "Processed 19/72: 25 (83 slices, 4 variations each)\n",
            "Processed 20/72: 27 (95 slices, 4 variations each)\n",
            "Processed 21/72: 28 (99 slices, 4 variations each)\n",
            "Processed 22/72: 29 (90 slices, 4 variations each)\n",
            "Processed 23/72: 3 (71 slices, 4 variations each)\n",
            "Processed 24/72: 30 (88 slices, 4 variations each)\n",
            "Processed 25/72: 32 (186 slices, 4 variations each)\n",
            "Processed 26/72: 33 (114 slices, 4 variations each)\n",
            "Processed 27/72: 36 (82 slices, 4 variations each)\n",
            "Processed 28/72: 37 (234 slices, 4 variations each)\n",
            "Processed 29/72: 44 (88 slices, 4 variations each)\n",
            "Processed 30/72: 45 (87 slices, 4 variations each)\n",
            "Processed 31/72: 46 (97 slices, 4 variations each)\n",
            "Processed 32/72: 47 (91 slices, 4 variations each)\n",
            "Processed 33/72: 49 (90 slices, 4 variations each)\n",
            "Processed 34/72: 51 (91 slices, 4 variations each)\n",
            "Processed 35/72: 57 (104 slices, 4 variations each)\n",
            "Processed 36/72: 59 (97 slices, 4 variations each)\n",
            "Processed 37/72: 6 (72 slices, 4 variations each)\n",
            "Processed 38/72: 61 (100 slices, 4 variations each)\n",
            "Processed 39/72: 62 (92 slices, 4 variations each)\n",
            "Processed 40/72: 63 (102 slices, 4 variations each)\n",
            "Processed 41/72: 64 (94 slices, 4 variations each)\n",
            "Processed 42/72: 65 (104 slices, 4 variations each)\n",
            "Processed 43/72: 66 (105 slices, 4 variations each)\n",
            "Processed 44/72: 67 (110 slices, 4 variations each)\n",
            "Processed 45/72: 68 (95 slices, 4 variations each)\n",
            "Processed 46/72: 69 (116 slices, 4 variations each)\n",
            "Processed 47/72: 7 (87 slices, 4 variations each)\n",
            "Processed 48/72: 70 (110 slices, 4 variations each)\n",
            "Processed 49/72: 71 (107 slices, 4 variations each)\n",
            "Processed 50/72: 72 (110 slices, 4 variations each)\n",
            "Processed 51/72: 73 (100 slices, 4 variations each)\n",
            "Processed 52/72: 74 (93 slices, 4 variations each)\n",
            "Processed 53/72: 75 (100 slices, 4 variations each)\n",
            "Processed 54/72: 77 (89 slices, 4 variations each)\n",
            "Processed 55/72: 78 (97 slices, 4 variations each)\n",
            "Processed 56/72: 79 (89 slices, 4 variations each)\n",
            "Processed 57/72: 8 (91 slices, 4 variations each)\n",
            "Processed 58/72: 81 (100 slices, 4 variations each)\n",
            "Processed 59/72: 82 (99 slices, 4 variations each)\n",
            "Processed 60/72: 85 (105 slices, 4 variations each)\n",
            "Processed 61/72: 87 (86 slices, 4 variations each)\n",
            "Processed 62/72: 88 (96 slices, 4 variations each)\n",
            "Processed 63/72: 9 (87 slices, 4 variations each)\n",
            "Processed 64/72: 90 (95 slices, 4 variations each)\n",
            "Processed 65/72: 91 (102 slices, 4 variations each)\n",
            "Processed 66/72: 92 (102 slices, 4 variations each)\n",
            "Processed 67/72: 94 (103 slices, 4 variations each)\n",
            "Processed 68/72: 95 (94 slices, 4 variations each)\n",
            "Processed 69/72: 96 (93 slices, 4 variations each)\n",
            "Processed 70/72: 97 (90 slices, 4 variations each)\n",
            "Processed 71/72: 98 (85 slices, 4 variations each)\n",
            "Processed 72/72: 99 (80 slices, 4 variations each)\n",
            "\n",
            "Finished processing all NIfTI files.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "\n",
        "# Define the base path for the output directory\n",
        "output_base_path = '/content/processed_data/'\n",
        "\n",
        "# Define subdirectories for images and masks\n",
        "output_images_path = os.path.join(output_base_path, 'images_2d/')\n",
        "output_masks_path = os.path.join(output_base_path, 'masks_2d/')\n",
        "\n",
        "# Create the directories if they do not already exist\n",
        "os.makedirs(output_images_path, exist_ok=True)\n",
        "os.makedirs(output_masks_path, exist_ok=True)\n",
        "\n",
        "# Define the base input paths for the NIfTI image and mask files\n",
        "image_input_path = '/content/drive/MyDrive/med_segmentation/pku_train_dataset/ct/'\n",
        "mask_input_path = '/content/drive/MyDrive/med_segmentation/pku_train_dataset/label/'\n",
        "\n",
        "# Rearrange the index\n",
        "pku_label_rearrange = {0:0, 1:1, 2:2, 3:3, 4:3, 5:4, 6:4, 7:5, 8:6, 9:7, 10:8, 11:9}\n",
        "\n",
        "# Get sorted lists of all .nii.gz files from both directories\n",
        "image_files_raw = sorted([f for f in os.listdir(image_input_path) if f.endswith('.nii.gz')])\n",
        "mask_files_raw = sorted([f for f in os.listdir(mask_input_path) if f.endswith('.nii.gz')])\n",
        "\n",
        "# Create dictionaries for easy lookup\n",
        "image_dict = {f.replace('.nii.gz', ''): f for f in image_files_raw}\n",
        "mask_dict = {f.replace('.nii.gz', ''): f for f in mask_files_raw}\n",
        "\n",
        "# Pair each image file with its corresponding mask file\n",
        "paired_files = []\n",
        "for base_name in sorted(image_dict.keys()):\n",
        "    if base_name in mask_dict:\n",
        "        image_full_path = os.path.join(image_input_path, image_dict[base_name])\n",
        "        mask_full_path = os.path.join(mask_input_path, mask_dict[base_name])\n",
        "        paired_files.append((image_full_path, mask_full_path))\n",
        "\n",
        "\n",
        "# Clipping values for Hounsfield Units\n",
        "min_clip_value = -800\n",
        "max_clip_value = 300\n",
        "\n",
        "print(f\"Starting processing of {len(paired_files)} image-mask pairs...\")\n",
        "\n",
        "processed_count = 0\n",
        "label_frequency = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0}\n",
        "\n",
        "for image_full_path, mask_full_path in paired_files:\n",
        "    # Extract base name for consistent file naming, e.g., '1' from '1.nii.gz'\n",
        "    base_name = os.path.basename(image_full_path).replace('.nii.gz', '')\n",
        "\n",
        "    try:\n",
        "        # Load 3D image and mask data into memory\n",
        "        image_nifti = nib.load(image_full_path)\n",
        "        mask_nifti = nib.load(mask_full_path)\n",
        "\n",
        "        image_data = image_nifti.get_fdata()\n",
        "        # Ensure mask data is integer type for labels, using np.int8 for memory efficiency\n",
        "        original_mask_data = mask_nifti.get_fdata().astype(np.int8)\n",
        "        max_key = max(pku_label_rearrange.keys())\n",
        "        lut = np.array([pku_label_rearrange[k] for k in range(max_key+1)])\n",
        "        mask_data = lut[original_mask_data]\n",
        "\n",
        "        # count the frequency of labels\n",
        "        unique, counts = np.unique(mask_data, return_counts=True)\n",
        "        if len(unique) != 1:\n",
        "            for u, c in zip(unique, counts):\n",
        "                label_frequency[u] += c\n",
        "\n",
        "        # Basic shape consistency check\n",
        "        if image_data.shape != mask_data.shape:\n",
        "            print(f\"Warning: Shape mismatch for {base_name}. Skipping. Image: {image_data.shape}, Mask: {mask_data.shape}\")\n",
        "            # Explicitly clear 3D data from memory for the current pair\n",
        "            del image_nifti, mask_nifti, image_data, mask_data\n",
        "            continue # Move to the next pair\n",
        "\n",
        "        num_slices = image_data.shape[2] # Number of slices along the depth (z) dimension\n",
        "\n",
        "        for z in range(num_slices):\n",
        "            # Extract 2D slices for current depth (z)\n",
        "            original_image_slice = image_data[:, :, z]\n",
        "            original_mask_slice = mask_data[:, :, z]\n",
        "\n",
        "            # --- Image Preprocessing: Clip and Rescale ---\n",
        "            # 1. Clip pixel values to the defined Hounsfield Unit range\n",
        "            clipped_image_slice = np.clip(original_image_slice, min_clip_value, max_clip_value)\n",
        "            # 2. Rescale clipped values to [0, 1] range\n",
        "            preprocessed_image_slice = (clipped_image_slice - min_clip_value) / (max_clip_value - min_clip_value)\n",
        "\n",
        "            # List to hold all variations (original + augmented) for the current z-slice\n",
        "            slices_to_save = []\n",
        "            # Add original (preprocessed) slices with rotation angle 0\n",
        "            slices_to_save.append((preprocessed_image_slice, original_mask_slice, 0))\n",
        "\n",
        "            # --- Data Augmentation: Rotations ---\n",
        "            # Apply 90, 180, and 270-degree rotations using np.rot90\n",
        "            for k, angle in enumerate([90, 180, 270]):\n",
        "                # k=0 for 90 deg, k=1 for 180 deg, k=2 for 270 deg\n",
        "                rotated_image_slice = np.rot90(preprocessed_image_slice, k=k+1)\n",
        "                rotated_mask_slice = np.rot90(original_mask_slice, k=k+1)\n",
        "                slices_to_save.append((rotated_image_slice, rotated_mask_slice, angle))\n",
        "\n",
        "            # --- Save all generated slices ---\n",
        "            for img_slice, msk_slice, angle in slices_to_save:\n",
        "                # Construct filenames including original base name, slice index, and rotation angle\n",
        "                image_filename = f\"{base_name}_slice_{z:03d}_rot_{angle}.npy\"\n",
        "                mask_filename = f\"{base_name}_slice_{z:03d}_rot_{angle}.npy\"\n",
        "\n",
        "                # Save image slices as float32 to preserve intensity information\n",
        "                np.save(os.path.join(output_images_path, image_filename), img_slice.astype(np.float32))\n",
        "                # Save mask slices as int8 to keep them as labels and be memory efficient\n",
        "                np.save(os.path.join(output_masks_path, mask_filename), msk_slice.astype(np.int8))\n",
        "\n",
        "        processed_count += 1\n",
        "        print(f\"Processed {processed_count}/{len(paired_files)}: {base_name} ({num_slices} slices, 4 variations each)\")\n",
        "\n",
        "        # Explicitly clear 3D data from memory for the current pair to avoid memory overflow\n",
        "        del image_nifti, mask_nifti, image_data, mask_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {base_name}: {e}\")\n",
        "        # Attempt to clear memory even on error to prevent issues for subsequent files\n",
        "        if 'image_nifti' in locals(): del image_nifti\n",
        "        if 'mask_nifti' in locals(): del mask_nifti\n",
        "        if 'image_data' in locals(): del image_data\n",
        "        if 'mask_data' in locals(): del mask_data\n",
        "        continue # Continue to the next file pair even if one fails\n",
        "\n",
        "print(\"\\nFinished processing all NIfTI files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4iHtHxOv5DMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c35e676-438d-4fa1-c76f-4c9e6955a1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting processing of 41 image-mask pairs...\n",
            "Processed 1/41: Case_00001_0000 (270 slices, 4 variations each)\n",
            "Processed 2/41: Case_00002_0000 (64 slices, 4 variations each)\n",
            "Processed 3/41: Case_00003_0000 (834 slices, 4 variations each)\n",
            "Processed 4/41: Case_00004_0000 (157 slices, 4 variations each)\n",
            "Processed 5/41: Case_00005_0000 (61 slices, 4 variations each)\n",
            "Processed 6/41: Case_00006_0000 (227 slices, 4 variations each)\n",
            "Processed 7/41: Case_00007_0000 (77 slices, 4 variations each)\n",
            "Processed 8/41: Case_00008_0000 (50 slices, 4 variations each)\n",
            "Processed 9/41: Case_00009_0000 (80 slices, 4 variations each)\n",
            "Processed 10/41: Case_00011_0000 (92 slices, 4 variations each)\n",
            "Processed 11/41: Case_00012_0000 (439 slices, 4 variations each)\n",
            "Processed 12/41: Case_00014_0000 (32 slices, 4 variations each)\n",
            "Processed 13/41: Case_00015_0000 (159 slices, 4 variations each)\n",
            "Processed 14/41: Case_00017_0000 (85 slices, 4 variations each)\n",
            "Processed 15/41: Case_00018_0000 (670 slices, 4 variations each)\n",
            "Processed 16/41: Case_00019_0000 (96 slices, 4 variations each)\n",
            "Processed 17/41: Case_00020_0000 (68 slices, 4 variations each)\n",
            "Processed 18/41: Case_00021_0000 (673 slices, 4 variations each)\n",
            "Processed 19/41: Case_00022_0000 (553 slices, 4 variations each)\n",
            "Processed 20/41: Case_00023_0000 (104 slices, 4 variations each)\n",
            "Processed 21/41: Case_00024_0000 (101 slices, 4 variations each)\n",
            "Processed 22/41: Case_00025_0000 (90 slices, 4 variations each)\n",
            "Processed 23/41: Case_00026_0000 (80 slices, 4 variations each)\n",
            "Processed 24/41: Case_00029_0000 (43 slices, 4 variations each)\n",
            "Processed 25/41: Case_00030_0000 (234 slices, 4 variations each)\n",
            "Processed 26/41: Case_00031_0000 (102 slices, 4 variations each)\n",
            "Processed 27/41: Case_00034_0000 (76 slices, 4 variations each)\n",
            "Processed 28/41: Case_00035_0000 (34 slices, 4 variations each)\n",
            "Processed 29/41: Case_00036_0000 (186 slices, 4 variations each)\n",
            "Processed 30/41: Case_00037_0000 (113 slices, 4 variations each)\n",
            "Processed 31/41: Case_00039_0000 (304 slices, 4 variations each)\n",
            "Processed 32/41: Case_00040_0000 (325 slices, 4 variations each)\n",
            "Processed 33/41: Case_00041_0000 (69 slices, 4 variations each)\n",
            "Processed 34/41: Case_00042_0000 (262 slices, 4 variations each)\n",
            "Processed 35/41: Case_00044_0000 (274 slices, 4 variations each)\n",
            "Processed 36/41: Case_00045_0000 (38 slices, 4 variations each)\n",
            "Processed 37/41: Case_00046_0000 (59 slices, 4 variations each)\n",
            "Processed 38/41: Case_00047_0000 (734 slices, 4 variations each)\n",
            "Processed 39/41: Case_00048_0000 (103 slices, 4 variations each)\n",
            "Processed 40/41: Case_00049_0000 (103 slices, 4 variations each)\n",
            "Processed 41/41: Case_00050_0000 (83 slices, 4 variations each)\n",
            "\n",
            "Finished processing all NIfTI files.\n"
          ]
        }
      ],
      "source": [
        "# Then, preprocess all the 'other' .nii.gz CT images & masks.\n",
        "# Define the base input paths for the NIfTI image and mask files\n",
        "image_input_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Image/'\n",
        "mask_input_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Mask/'\n",
        "\n",
        "# Rearrange the index\n",
        "other_label_rearrange = {0:0, 1:7, 2:6, 3:10, 4:11}\n",
        "\n",
        "# Get sorted lists of all .nii.gz files from both directories\n",
        "image_files_raw = sorted([f for f in os.listdir(image_input_path) if f.endswith('.nii.gz')])\n",
        "mask_files_raw = sorted([f for f in os.listdir(mask_input_path) if f.endswith('.nii.gz')])\n",
        "\n",
        "# Create dictionaries for easy lookup\n",
        "image_dict = {f.replace('_0000.nii.gz', ''): f for f in image_files_raw}\n",
        "mask_dict = {f.replace('.nii.gz', ''): f for f in mask_files_raw}\n",
        "\n",
        "# Pair each image file with its corresponding mask file\n",
        "paired_files = []\n",
        "for base_name in sorted(image_dict.keys()):\n",
        "    if base_name in mask_dict:\n",
        "        image_full_path = os.path.join(image_input_path, image_dict[base_name])\n",
        "        mask_full_path = os.path.join(mask_input_path, mask_dict[base_name])\n",
        "        paired_files.append((image_full_path, mask_full_path))\n",
        "\n",
        "\n",
        "# Clipping values for Hounsfield Units\n",
        "min_clip_value = -800\n",
        "max_clip_value = 300\n",
        "\n",
        "print(f\"Starting processing of {len(paired_files)} image-mask pairs...\")\n",
        "\n",
        "processed_count = 0\n",
        "\n",
        "for image_full_path, mask_full_path in paired_files:\n",
        "    # Extract base name for consistent file naming, e.g., '1' from '1.nii.gz'\n",
        "    base_name = os.path.basename(image_full_path).replace('.nii.gz', '')\n",
        "\n",
        "    try:\n",
        "        # Load 3D image and mask data into memory\n",
        "        image_nifti = nib.load(image_full_path)\n",
        "        mask_nifti = nib.load(mask_full_path)\n",
        "\n",
        "        image_data = image_nifti.get_fdata()\n",
        "        # Ensure mask data is integer type for labels, using np.int8 for memory efficiency\n",
        "        original_mask_data = mask_nifti.get_fdata().astype(np.int8)\n",
        "        max_key = max(other_label_rearrange.keys())\n",
        "        lut = np.array([other_label_rearrange[k] for k in range(max_key+1)])\n",
        "        mask_data = lut[original_mask_data]\n",
        "\n",
        "        # count the frequency of labels\n",
        "        unique, counts = np.unique(mask_data, return_counts=True)\n",
        "        if len(unique) != 1:\n",
        "            for u, c in zip(unique, counts):\n",
        "                label_frequency[u] += c\n",
        "\n",
        "        # Basic shape consistency check\n",
        "        if image_data.shape != mask_data.shape:\n",
        "            print(f\"Warning: Shape mismatch for {base_name}. Skipping. Image: {image_data.shape}, Mask: {mask_data.shape}\")\n",
        "            # Explicitly clear 3D data from memory for the current pair\n",
        "            del image_nifti, mask_nifti, image_data, mask_data\n",
        "            continue # Move to the next pair\n",
        "\n",
        "        num_slices = image_data.shape[2] # Number of slices along the depth (z) dimension\n",
        "\n",
        "        for z in range(num_slices):\n",
        "            # Extract 2D slices for current depth (z)\n",
        "            original_image_slice = image_data[:, :, z]\n",
        "            original_mask_slice = mask_data[:, :, z]\n",
        "\n",
        "            # --- Image Preprocessing: Clip and Rescale ---\n",
        "            # 1. Clip pixel values to the defined Hounsfield Unit range\n",
        "            clipped_image_slice = np.clip(original_image_slice, min_clip_value, max_clip_value)\n",
        "            # 2. Rescale clipped values to [0, 1] range\n",
        "            preprocessed_image_slice = (clipped_image_slice - min_clip_value) / (max_clip_value - min_clip_value)\n",
        "\n",
        "            # List to hold all variations (original + augmented) for the current z-slice\n",
        "            slices_to_save = []\n",
        "            # Add original (preprocessed) slices with rotation angle 0\n",
        "            slices_to_save.append((preprocessed_image_slice, original_mask_slice, 0))\n",
        "\n",
        "            # --- Data Augmentation: Rotations ---\n",
        "            # Apply 90, 180, and 270-degree rotations using np.rot90\n",
        "            for k, angle in enumerate([90, 180, 270]):\n",
        "                # k=0 for 90 deg, k=1 for 180 deg, k=2 for 270 deg\n",
        "                rotated_image_slice = np.rot90(preprocessed_image_slice, k=k+1)\n",
        "                rotated_mask_slice = np.rot90(original_mask_slice, k=k+1)\n",
        "                slices_to_save.append((rotated_image_slice, rotated_mask_slice, angle))\n",
        "\n",
        "            # --- Save all generated slices ---\n",
        "            for img_slice, msk_slice, angle in slices_to_save:\n",
        "                # Construct filenames including original base name, slice index, and rotation angle\n",
        "                # Notice to distinguish 'other' from 'pku' in filenames\n",
        "                image_filename = f\"other_{base_name}_slice_{z:03d}_rot_{angle}.npy\"\n",
        "                mask_filename = f\"other_{base_name}_slice_{z:03d}_rot_{angle}.npy\"\n",
        "\n",
        "                # Save image slices as float32 to preserve intensity information\n",
        "                np.save(os.path.join(output_images_path, image_filename), img_slice.astype(np.float32))\n",
        "                # Save mask slices as int8 to keep them as labels and be memory efficient\n",
        "                np.save(os.path.join(output_masks_path, mask_filename), msk_slice.astype(np.int8))\n",
        "\n",
        "        processed_count += 1\n",
        "        print(f\"Processed {processed_count}/{len(paired_files)}: {base_name} ({num_slices} slices, 4 variations each)\")\n",
        "\n",
        "        # Explicitly clear 3D data from memory for the current pair to avoid memory overflow\n",
        "        del image_nifti, mask_nifti, image_data, mask_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {base_name}: {e}\")\n",
        "        # Attempt to clear memory even on error to prevent issues for subsequent files\n",
        "        if 'image_nifti' in locals(): del image_nifti\n",
        "        if 'mask_nifti' in locals(): del mask_nifti\n",
        "        if 'image_data' in locals(): del image_data\n",
        "        if 'mask_data' in locals(): del mask_data\n",
        "        continue # Continue to the next file pair even if one fails\n",
        "\n",
        "print(\"\\nFinished processing all NIfTI files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MTGjqazJAh9"
      },
      "source": [
        "Build the U-net. Input: [Batch, 1, 512, 512]. Output: [Batch, 12, 512, 512]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "888b076d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=12, features=[64, 128, 256]):\n",
        "        super(UNet, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Downsampling Path (Encoder)\n",
        "        for feature in features:\n",
        "            self.downs.append(self._conv_block(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._conv_block(features[-1], features[-1] * 2)\n",
        "\n",
        "        # Upsampling Path (Decoder)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(self._conv_block(feature * 2, feature))\n",
        "\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def _conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            # nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            # nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        # Encoder\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Decoder\n",
        "        skip_connections = skip_connections[::-1] # Reverse the skip connections for the decoder path\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            # If dimensions don't match (due to padding in conv or odd input sizes), resize skip connection\n",
        "            # Assuming [batch, channel, height, width] for x and skip_connection\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:], mode='bilinear', align_corners=True)\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR7Iz3-9I88n"
      },
      "source": [
        "Next, we can collect the .npy files into datasets and dataloaders for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7b4f2ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b422b0e7-ee62-43ab-f6c6-dace72682a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 60496 paired (image+mask) files.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "image_files = sorted([os.path.join(output_images_path, f) for f in os.listdir(output_images_path) if f.endswith('.npy')])\n",
        "mask_files = sorted([os.path.join(output_masks_path, f) for f in os.listdir(output_masks_path) if f.endswith('.npy')])\n",
        "\n",
        "# Create a dictionary for mask files for efficient lookup and pairing\n",
        "mask_dict = {os.path.basename(f): f for f in mask_files}\n",
        "\n",
        "paired_file_paths = []\n",
        "for img_path in image_files:\n",
        "    img_filename = os.path.basename(img_path)\n",
        "    if img_filename in mask_dict:\n",
        "        paired_file_paths.append((img_path, mask_dict[img_filename]))\n",
        "\n",
        "print(f\"Found {len(paired_file_paths)} paired (image+mask) files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "87b388c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7598da61-5068-4684-d163-d5c636f50646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NPYDataset class defined.\n"
          ]
        }
      ],
      "source": [
        "class NPYDataset(Dataset):\n",
        "    def __init__(self, file_paths):\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, mask_path = self.file_paths[idx]\n",
        "\n",
        "        # Load image and mask data\n",
        "        image = np.load(image_path)\n",
        "        mask = np.load(mask_path)\n",
        "\n",
        "        # Add a channel dimension: (H, W) -> (1, H, W)\n",
        "        image = np.expand_dims(image, axis=0) # Add channel dimension\n",
        "        mask = np.expand_dims(mask, axis=0)   # Add channel dimension\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        # Image should be float32 for model input\n",
        "        image_tensor = torch.from_numpy(image).float()\n",
        "        # Mask should be long for CrossEntropyLoss (if applicable) or other label-based loss functions\n",
        "        mask_tensor = torch.from_numpy(mask).long()\n",
        "\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "print(\"NPYDataset class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0ef58fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddd57d0-0683-47f5-9e18-1c824cd5a02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 48396\n",
            "Number of validation samples: 12100\n",
            "Training DataLoader created with batch size: 64\n",
            "Validation DataLoader created with batch size: 64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Split the paired file paths into training and validation sets\n",
        "train_files, val_files = train_test_split(paired_file_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = NPYDataset(train_files)\n",
        "val_dataset = NPYDataset(val_files)\n",
        "\n",
        "# Define DataLoader parameters\n",
        "batch_size = 64 # You can adjust this based on your GPU memory\n",
        "num_workers = 2 # Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n",
        "\n",
        "# Create DataLoader instances\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Training DataLoader created with batch size: {batch_size}\")\n",
        "print(f\"Validation DataLoader created with batch size: {batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aapNYEdaXqoo"
      },
      "source": [
        "I'll define the loss function. I'm not sure whether dice loss or cross entropy loss(sum over all pixels) is better. So let's first make both!\n",
        "\n",
        "IOU is intersection over union, which is used to evaluate the accuracy, not loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9440a878"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Dice Loss and Dice Score (for evaluation)\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, num_classes=None):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # pred: (N, C, H, W), target: (N, 1, H, W)\n",
        "        # Ensure target is one-hot encoded for multi-class dice loss\n",
        "        # Squeeze channel dimension if it exists: (N, 1, H, W) -> (N, H, W)\n",
        "        target_one_hot = F.one_hot(target.squeeze(1), num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        # Apply softmax to predictions to get probabilities\n",
        "        pred = F.softmax(pred, dim=1)\n",
        "\n",
        "        # Flatten label and prediction tensors\n",
        "        pred = pred.contiguous().view(pred.shape[0], self.num_classes, -1)\n",
        "        target_one_hot = target_one_hot.contiguous().view(target_one_hot.shape[0], self.num_classes, -1)\n",
        "\n",
        "        # Compute Dice coefficient for each class\n",
        "        intersection = (pred * target_one_hot).sum(dim=2)\n",
        "        union = pred.sum(dim=2) + target_one_hot.sum(dim=2)\n",
        "        dice = (2. * intersection + 1e-8) / (union + 1e-8) # Add epsilon for numerical stability\n",
        "\n",
        "        # Average Dice loss over foreground classes (excluding background class 0)\n",
        "        # Handle partial annotations: some slices may not be labeled even if organs exist\n",
        "        dice_foreground = dice[:, 1:]  # Exclude background (index 0)\n",
        "        # Check where the target actually has foreground annotations\n",
        "        target_foreground = target_one_hot[:, 1:, :]  # (N, 11, H*W)\n",
        "        target_has_foreground = target_foreground.sum(dim=2) > 1e-6  # (N, 11) - True where target has this organ\n",
        "        # Only compute loss for classes where target is annotated (has foreground)\n",
        "        # For unannotated classes (all background in target), set dice=1.0 (no penalty)\n",
        "        dice_foreground = torch.where(target_has_foreground, dice_foreground, torch.ones_like(dice_foreground))\n",
        "\n",
        "        # # Direct average without weights between organs\n",
        "        # loss = 1 - dice_foreground.mean() # Average over foreground classes and batch for loss\n",
        "\n",
        "        # Weighted loss, also excluding background\n",
        "        weights = np.array([label_frequency[a] for a in range(1,12)], dtype=float)\n",
        "        weights = weights**(-1)\n",
        "        weights = weights / weights.sum()\n",
        "        weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "        loss = 1 - (dice_foreground * weights).sum(dim=1).mean() # Weighted average over organs, and averaged over batch for loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "def calculate_dice_score(pred, target, num_classes):\n",
        "    # pred: (N, C, H, W), target: (N, 1, H, W)\n",
        "    # Convert target to one-hot encoding\n",
        "    target_one_hot = F.one_hot(target.squeeze(1), num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "\n",
        "    # Get predictions by taking argmax for multi-class\n",
        "    pred_labels = torch.argmax(F.softmax(pred, dim=1), dim=1)\n",
        "    pred_one_hot = F.one_hot(pred_labels, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "\n",
        "    # Flatten label and prediction tensors\n",
        "    pred_one_hot = pred_one_hot.contiguous().view(pred_one_hot.shape[0], num_classes, -1)\n",
        "    target_one_hot = target_one_hot.contiguous().view(target_one_hot.shape[0], num_classes, -1)\n",
        "\n",
        "    # Compute Dice coefficient for each class\n",
        "    intersection = (pred_one_hot * target_one_hot).sum(dim=2) # Sum along H*W for each class and each batch\n",
        "    union = pred_one_hot.sum(dim=2) + target_one_hot.sum(dim=2)\n",
        "\n",
        "    dice_scores = (2. * intersection + 1e-8) / (union + 1e-8)\n",
        "    return dice_scores.mean(dim=0) # Return average Dice per class across the batch (num_classes,)\n",
        "\n",
        "# Perhaps this will not be used because it is similar to dice score\n",
        "# Dice score is usually used in medical image segmentation\n",
        "def calculate_iou(pred, target, num_classes):\n",
        "    # pred: (N, C, H, W), target: (N, 1, H, W)\n",
        "\n",
        "    # Get predicted labels by taking argmax for multi-class\n",
        "    pred_labels = torch.argmax(F.softmax(pred, dim=1), dim=1) # (N, H, W)\n",
        "\n",
        "    # Convert target and pred_labels to one-hot encoded tensors\n",
        "    # target_one_hot: (N, H, W) -> (N, num_classes, H, W)\n",
        "    target_one_hot = F.one_hot(target.squeeze(1), num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    # pred_one_hot: (N, H, W) -> (N, num_classes, H, W)\n",
        "    pred_one_hot = F.one_hot(pred_labels, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "\n",
        "    # Flatten these one-hot encoded tensors to (N, num_classes, H*W)\n",
        "    pred_flat = pred_one_hot.contiguous().view(pred_one_hot.shape[0], num_classes, -1)\n",
        "    target_flat = target_one_hot.contiguous().view(target_one_hot.shape[0], num_classes, -1)\n",
        "\n",
        "    # For each class, calculate the intersection and union\n",
        "    intersection = (pred_flat * target_flat).sum(dim=2) # (N, num_classes)\n",
        "    union = (pred_flat + target_flat - pred_flat * target_flat).sum(dim=2) # (N, num_classes)\n",
        "\n",
        "    # Compute the IoU for each class\n",
        "    iou_scores = (intersection + 1e-8) / (union + 1e-8) # (N, num_classes)\n",
        "\n",
        "    # Average the IoU scores over the batch dimension (dim=0) to get (num_classes,)\n",
        "    return iou_scores.mean(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycmg1uHQdfJD"
      },
      "source": [
        "Next, train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xocnj0q5RcPf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90abe2ee-ddf6-4625-8140-42280a860fb0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n",
            "\n",
            "Training UNet model on cuda for 10 epochs...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train/step_ce_loss</td><td></td></tr><tr><td>train/step_dice/class_0</td><td></td></tr><tr><td>train/step_dice/class_1</td><td></td></tr><tr><td>train/step_dice/class_10</td><td></td></tr><tr><td>train/step_dice/class_11</td><td></td></tr><tr><td>train/step_dice/class_2</td><td></td></tr><tr><td>train/step_dice/class_3</td><td></td></tr><tr><td>train/step_dice/class_4</td><td></td></tr><tr><td>train/step_dice/class_5</td><td></td></tr><tr><td>+20</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train/step_ce_loss</td><td>0.22405</td></tr><tr><td>train/step_dice/class_0</td><td>0.96783</td></tr><tr><td>train/step_dice/class_1</td><td>1</td></tr><tr><td>train/step_dice/class_10</td><td>0.75</td></tr><tr><td>train/step_dice/class_11</td><td>0.91667</td></tr><tr><td>train/step_dice/class_2</td><td>0.83333</td></tr><tr><td>train/step_dice/class_3</td><td>1</td></tr><tr><td>train/step_dice/class_4</td><td>0.91667</td></tr><tr><td>train/step_dice/class_5</td><td>1</td></tr><tr><td>+20</td><td>...</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unet-medical-seg-20251205-061023</strong> at: <a href='https://wandb.ai/smileyao2023-peking-university/medical-segmentation/runs/p0dxcc7j' target=\"_blank\">https://wandb.ai/smileyao2023-peking-university/medical-segmentation/runs/p0dxcc7j</a><br> View project at: <a href='https://wandb.ai/smileyao2023-peking-university/medical-segmentation' target=\"_blank\">https://wandb.ai/smileyao2023-peking-university/medical-segmentation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251205_061031-p0dxcc7j/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251205_062510-0ugbilfk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/smileyao2023-peking-university/medical-segmentation/runs/0ugbilfk' target=\"_blank\">unet-medical-seg-20251205-062510</a></strong> to <a href='https://wandb.ai/smileyao2023-peking-university/medical-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/smileyao2023-peking-university/medical-segmentation' target=\"_blank\">https://wandb.ai/smileyao2023-peking-university/medical-segmentation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/smileyao2023-peking-university/medical-segmentation/runs/0ugbilfk' target=\"_blank\">https://wandb.ai/smileyao2023-peking-university/medical-segmentation/runs/0ugbilfk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch [1/10], Batch [100/757], CE Loss: 0.6819, Dice Loss: 0.0978\n",
            "  Epoch [1/10], Batch [200/757], CE Loss: 0.1436, Dice Loss: 0.0965\n",
            "  Epoch [1/10], Batch [300/757], CE Loss: 0.1338, Dice Loss: 0.1013\n",
            "  Epoch [1/10], Batch [400/757], CE Loss: 0.1306, Dice Loss: 0.0959\n",
            "  Epoch [1/10], Batch [500/757], CE Loss: 0.1310, Dice Loss: 0.0927\n",
            "  Epoch [1/10], Batch [600/757], CE Loss: 0.1259, Dice Loss: 0.0990\n",
            "  Epoch [1/10], Batch [700/757], CE Loss: 0.1263, Dice Loss: 0.0968\n",
            "\n",
            "Epoch [1/10]\n",
            "  Val Loss:   0.2374, Val Dice Loss:   0.0961, Val CE Loss:   0.1413\n",
            "  avg Val IoU:   0.8753, avg Val Dice:   0.8765\n",
            "  Val IoU:  [0.96930475 0.93092105 0.87754934 0.94333882 0.95501645 0.95921053\n",
            " 0.70616776 0.58659539 0.86570724 0.97919408 0.86381579 0.86677632], Val Dice:  [0.98392173 0.93092105 0.87754934 0.94333882 0.95501645 0.95921053\n",
            " 0.70616776 0.58659539 0.86570724 0.97919408 0.86381579 0.86677632]\n",
            "  Model saved to /content/drive/MyDrive/med_segmentation/best_unet_model.pth with improved validation loss: 0.2374\n",
            "  Epoch [2/10], Batch [100/757], CE Loss: 0.1267, Dice Loss: 0.0977\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import wandb\n",
        "import os\n",
        "import datetime\n",
        "from google.colab import userdata\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "scaler = GradScaler()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "run_name = f\"unet-medical-seg-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "num_classes = 12 # Based on unique labels from 0 to 11\n",
        "\n",
        "# 1. Initialize the UNet model and move to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  print('Using cuda\\n')\n",
        "\n",
        "# Define the loss function\n",
        "# Combine CrossEntropyLoss (for classification) and DiceLoss (for segmentation quality)\n",
        "# CrossEntropyLoss requires target to be (N, H, W), not (N, 1, H, W), so we'll squeeze it.\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "criterion_dice = DiceLoss(num_classes=num_classes)\n",
        "dice_weight = 1.0  # total weight = CE + dice_weight * Dice\n",
        "\n",
        "# Training parameters\n",
        "features=[64, 128, 256]  # When changing this, don't forget to also change that in visualization\n",
        "num_epochs = 10 # You can adjust this\n",
        "save_path = '/content/drive/MyDrive/med_segmentation/best_unet_model.pth'\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Define the optimizer and scheduler\n",
        "model = UNet(in_channels=1, out_channels=num_classes, features=features).to(device)\n",
        "lr = 2e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "print(f\"Training UNet model on {device} for {num_epochs} epochs...\")\n",
        "\n",
        "try:\n",
        "  wandb_api_key = userdata.get(\"WANDB_API_KEY\")\n",
        "except:\n",
        "  raise ValueError('Please set the API key in Colab Secrets!')\n",
        "os.environ['WANDB_API_KEY'] = wandb_api_key\n",
        "\n",
        "# Reduce batch size for training and validation DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "wandb.init(\n",
        "    project=\"medical-segmentation\",\n",
        "    name=run_name,  # Unique name for each run\n",
        "    config={\n",
        "        \"learning_rate\": lr,\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": train_loader.batch_size,\n",
        "        \"features\": features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"optimizer\": optimizer.__class__.__name__,\n",
        "        \"scheduler\": scheduler.__class__.__name__,\n",
        "        \"loss_functions\": [\"CrossEntropyLoss\", \"DiceLoss\"],\n",
        "        \"Dice weight\": dice_weight,\n",
        "        \"clip value\": [min_clip_value, max_clip_value],\n",
        "        \"organ weight\": 'true'\n",
        "    })\n",
        "\n",
        "N_record = 100  # log after each N_record batches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss_dice = 0.0\n",
        "    running_loss_ce = 0.0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device) # Masks are already long type from NPYDataset\n",
        "        optimizer.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(images)\n",
        "                ce_loss = criterion_ce(outputs, masks.squeeze(1)) # squeeze channel dim\n",
        "                dice_loss = criterion_dice(outputs, masks)  # Calculate DiceLoss (target expects N, 1, H, W)\n",
        "                loss = ce_loss + dice_weight * dice_loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        # To calculate the average loss over the N_record batches\n",
        "        running_loss_ce += ce_loss.item()\n",
        "        running_loss_dice += dice_loss.item()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate scores for each organ\n",
        "        step_iou = calculate_iou(outputs.detach(), masks.detach(), num_classes).cpu().numpy()\n",
        "        step_dice_score = calculate_dice_score(outputs.detach(), masks.detach(), num_classes).cpu().numpy()\n",
        "\n",
        "        # Calculate loss averaged over num_batches (it's already averaged)\n",
        "        wandb.log(\n",
        "            {\n",
        "            \"train/step_loss\": loss.item(),\n",
        "            \"train/step_ce_loss\": ce_loss.item(),\n",
        "            \"train/step_dice_loss\": dice_loss.item()\n",
        "            }\n",
        "        )\n",
        "        # Calculate scores averaged on all organs\n",
        "        # Here I take average between organs without weights\n",
        "        wandb.log(\n",
        "            {\n",
        "            \"train/step_iou_mean\": step_iou.mean(),\n",
        "            \"train/step_dice_score_mean\": step_dice_score.mean()\n",
        "            }\n",
        "        )\n",
        "        # Calculate scores for each class\n",
        "        log_dict = {\"epoch\": epoch + 1}\n",
        "        for class_idx in range(num_classes):\n",
        "            log_dict[f\"train/step_iou/class_{class_idx}\"] = step_iou[class_idx]\n",
        "            log_dict[f\"train/step_dice/class_{class_idx}\"] = step_dice_score[class_idx]\n",
        "        wandb.log(log_dict)\n",
        "\n",
        "        if (batch_idx + 1) % N_record == 0: # Print every N_record batches\n",
        "            print(f\"  Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], CE Loss: {running_loss_ce/N_record:.4f}, Dice Loss: {running_loss_dice/N_record:.4f}\")\n",
        "\n",
        "            running_loss_dice = 0.0\n",
        "            running_loss_ce = 0.0\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_loss_ce = 0.0\n",
        "    val_loss_dice = 0.0\n",
        "    val_iou_scores = np.zeros(num_classes)\n",
        "    val_dice_scores = np.zeros(num_classes)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            ce_loss_val = criterion_ce(outputs, masks.squeeze(1))\n",
        "            dice_loss_val = criterion_dice(outputs, masks)\n",
        "            loss_val = ce_loss_val + dice_weight * dice_loss_val\n",
        "            val_loss_ce += ce_loss_val.item()\n",
        "            val_loss_dice += dice_loss_val.item()\n",
        "            val_loss += loss_val.item()\n",
        "\n",
        "            step_iou = calculate_iou(outputs.detach(), masks.detach(), num_classes).cpu().numpy()\n",
        "            val_iou_scores += step_iou\n",
        "            step_dice_score = calculate_dice_score(outputs.detach(), masks.detach(), num_classes).cpu().numpy()\n",
        "            val_dice_scores += step_dice_score\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_loss_ce = val_loss_ce / len(val_loader)\n",
        "    avg_loss_dice = val_loss_dice / len(val_loader)\n",
        "    avg_val_iou = val_iou_scores.mean() / len(val_loader)\n",
        "    avg_val_dice = val_dice_scores.mean() / len(val_loader)  # take plain average over organs, and then over batches\n",
        "\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\\n\" \\\n",
        "          f\"  Val Loss:   {avg_loss:.4f}, Val Dice Loss:   {avg_loss_dice:.4f}, Val CE Loss:   {avg_loss_ce:.4f}\\n\"\\\n",
        "          f\"  avg Val IoU:   {avg_val_iou:.4f}, avg Val Dice:   {avg_val_dice:.4f}\\n\"\\\n",
        "          f\"  Val IoU:  {val_iou_scores/len(val_loader)}, Val Dice:  {val_dice_scores/len(val_loader)}\")\n",
        "\n",
        "    # Save the model if validation loss improved\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"  Model saved to {save_path} with improved validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Log the validation per epoch\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"val/avg_loss\": avg_loss,\n",
        "        \"val/avg_loss_ce\": avg_loss_ce,\n",
        "        \"val/avg_loss_dice\": avg_loss_dice,\n",
        "        \"val/avg_iou\": avg_val_iou,\n",
        "        \"val/avg_dice\": avg_val_dice,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "\n",
        "    # Calculate scores for each class averaged over the validation batches\n",
        "    log_dict = {\"epoch\": epoch + 1}\n",
        "    for class_idx in range(num_classes):\n",
        "        log_dict[f\"val/iou/class_{class_idx}\"] = val_iou_scores[class_idx]/len(val_loader)\n",
        "        log_dict[f\"val/dice/class_{class_idx}\"] = val_dice_scores[class_idx]/len(val_loader)\n",
        "    wandb.log(log_dict)\n",
        "\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "wandb.finish()\n",
        "\n",
        "# --- Post-training cleanup and CPU transition ---\n",
        "\n",
        "# 1. Clear GPU memory by deleting the model and related tensors\n",
        "#    and then explicitly emptying the CUDA cache.\n",
        "del model, images, masks, outputs # Delete model and any tensors currently on GPU\n",
        "torch.cuda.empty_cache() # Clear any cached memory\n",
        "\n",
        "print(\"GPU memory cleared. Further operations will effectively run on CPU unless explicitly moved to GPU.\")\n",
        "\n",
        "# 2. Example: Loading the best model checkpoint onto CPU\n",
        "#    If you need to use the model again after training, load it onto the CPU device.\n",
        "\n",
        "# Create a new model instance (same architecture as during training)\n",
        "# new_model_cpu = UNet(in_channels=1, out_channels=num_classes, features=features)\n",
        "\n",
        "# Load the state dictionary from the saved path onto the CPU\n",
        "# new_model_cpu.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "# new_model_cpu.eval()\n",
        "\n",
        "# print(f\"Trained model loaded onto CPU from {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8a482f6"
      },
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnect the current runtime\n",
        "# This will terminate the Colab session, which is useful for releasing GPU resources\n",
        "# after a long training run.\n",
        "print(\"Notebook execution complete. Disconnecting runtime...\")\n",
        "runtime.unassign()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Model Predictions\n",
        "\n",
        "Now let's load the trained model and visualize its predictions on CT images. We'll display the original image, ground truth mask, and predicted mask side by side with a scrollable interface to explore all slices."
      ],
      "metadata": {
        "id": "peWeoCiYE3v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=12\n",
        "# Load the trained model\n",
        "model_path = '/content/best_unet_model.pth'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model architecture (same as during training)\n",
        "model = UNet(in_channels=1, out_channels=num_classes, features=[64, 128, 256]).to(device)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded from {model_path}\")\n",
        "print(f\"Model is on device: {device}\")"
      ],
      "metadata": {
        "id": "KbXe0qwAE7Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "# Load a CT image and its corresponding mask for visualization\n",
        "# Choose a file from the PKU dataset for visualization\n",
        "ct_image_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Image/Case_00002_0000.nii.gz'\n",
        "mask_path = '/content/drive/MyDrive/med_segmentation/other_train_dataset/Mask/Case_00002.nii.gz'\n",
        "\n",
        "# Load the 3D volumes\n",
        "ct_img = nib.load(ct_image_path)\n",
        "mask_img = nib.load(mask_path)\n",
        "\n",
        "ct_data = ct_img.get_fdata()\n",
        "mask_data_original = mask_img.get_fdata().astype(np.int8)\n",
        "\n",
        "# Apply the same preprocessing and label rearrangement as during training\n",
        "# pku_label_rearrange = {0:0, 1:1, 2:2, 3:3, 4:3, 5:4, 6:4, 7:5, 8:6, 9:7, 10:8, 11:9}\n",
        "other_label_rearrange = {0:0, 1:7, 2:6, 3:10, 4:11}\n",
        "max_key = max(other_label_rearrange.keys())\n",
        "lut = np.array([other_label_rearrange[k] for k in range(max_key+1)])\n",
        "mask_data = lut[mask_data_original]\n",
        "\n",
        "# Apply the same clipping and normalization as during training\n",
        "min_clip_value = -800\n",
        "max_clip_value = 300\n",
        "ct_data_clipped = np.clip(ct_data, min_clip_value, max_clip_value)\n",
        "ct_data_normalized = (ct_data_clipped - min_clip_value) / (max_clip_value - min_clip_value)\n",
        "\n",
        "print(f\"CT data shape: {ct_data_normalized.shape}\")\n",
        "print(f\"Mask data shape: {mask_data.shape}\")\n",
        "print(f\"Number of slices: {ct_data_normalized.shape[2]}\")"
      ],
      "metadata": {
        "id": "U-A3OURgE9Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for all slices\n",
        "num_slices = ct_data_normalized.shape[2]\n",
        "predicted_masks = []\n",
        "\n",
        "print(\"Generating predictions for all slices...\")\n",
        "with torch.no_grad():\n",
        "    for z in range(num_slices):\n",
        "        # Extract 2D slice\n",
        "        slice_2d = ct_data_normalized[:, :, z]\n",
        "\n",
        "        # Convert to tensor and add batch and channel dimensions\n",
        "        slice_tensor = torch.from_numpy(slice_2d).float().unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get model prediction\n",
        "        output = model(slice_tensor)\n",
        "\n",
        "        # Convert output to predicted class (argmax over class dimension)\n",
        "        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "        predicted_masks.append(pred_mask)\n",
        "\n",
        "predicted_masks = np.stack(predicted_masks, axis=2)\n",
        "print(f\"Predictions generated. Shape: {predicted_masks.shape}\")"
      ],
      "metadata": {
        "id": "2Lc_hZ9dFAF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define organ labels and colors for visualization\n",
        "organ_labels = {\n",
        "    0: 'Background',\n",
        "    1: 'Bladder',\n",
        "    2: 'Colon',\n",
        "    3: 'Femur Head',\n",
        "    4: 'Kidney',\n",
        "    5: 'Liver',\n",
        "    6: 'Rectum',\n",
        "    7: 'SmallIntestine',\n",
        "    8: 'SpinalCord',\n",
        "    9: 'Stomach',\n",
        "    10: 'Spleen',\n",
        "    11: 'Pancreas'\n",
        "}\n",
        "\n",
        "# Create a colormap for the different organs\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Define distinct colors for each organ class\n",
        "colors = ['black', 'red', 'blue', 'green', 'yellow', 'cyan',\n",
        "          'magenta', 'orange', 'purple', 'pink', 'brown', 'gray']\n",
        "cmap = mcolors.ListedColormap(colors)\n",
        "bounds = np.arange(num_classes + 1) - 0.5\n",
        "norm = mcolors.BoundaryNorm(bounds, cmap.N)"
      ],
      "metadata": {
        "id": "oUksydD6FE2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e32ce3ac"
      },
      "source": [
        "# create interactive commparison figures\n",
        "from ipywidgets import interact, IntSlider, fixed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_slice(slice_idx, ct_data, gt_mask, pred_mask, cmap, norm):\n",
        "    \"\"\"\n",
        "    Visualize a single slice with CT image, ground truth mask, and predicted mask\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Plot CT image\n",
        "    axes[0].imshow(ct_data[:, :, slice_idx], cmap='gray', vmin=0, vmax=1)\n",
        "    axes[0].set_title(f'CT Image - Slice {slice_idx + 1}/{ct_data.shape[2]}', fontsize=14)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Plot ground truth mask\n",
        "    im1 = axes[1].imshow(gt_mask[:, :, slice_idx], cmap=cmap, norm=norm, interpolation='nearest')\n",
        "    axes[1].set_title('Ground Truth Mask', fontsize=14)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Plot predicted mask\n",
        "    im2 = axes[2].imshow(pred_mask[:, :, slice_idx], cmap=cmap, norm=norm, interpolation='nearest')\n",
        "    axes[2].set_title('Predicted Mask', fontsize=14)\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Add a single colorbar for the predicted mask, associated only with axes[2]\n",
        "    cbar = fig.colorbar(im2, ax=axes[2], orientation='vertical', fraction=0.046, pad=0.04)\n",
        "    cbar.set_label('Organ Class', rotation=270, labelpad=15)\n",
        "    cbar.set_ticks(range(num_classes))\n",
        "    cbar.set_ticklabels([organ_labels[i] for i in range(num_classes)], fontsize=8)\n",
        "\n",
        "    # Calculate and display IoU and Dice score for this slice\n",
        "    gt_slice = gt_mask[:, :, slice_idx].flatten()\n",
        "    pred_slice = pred_mask[:, :, slice_idx].flatten()\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    unique_classes = np.unique(np.concatenate([gt_slice, pred_slice]))\n",
        "    slice_iou = []\n",
        "    slice_dice = []\n",
        "\n",
        "    for cls in unique_classes:\n",
        "        gt_cls = (gt_slice == cls)\n",
        "        pred_cls = (pred_slice == cls)\n",
        "        intersection = np.sum(gt_cls & pred_cls)\n",
        "        union = np.sum(gt_cls | pred_cls)\n",
        "\n",
        "        if union > 0:\n",
        "            iou = intersection / union\n",
        "            dice = (2 * intersection) / (np.sum(gt_cls) + np.sum(pred_cls))\n",
        "        else:\n",
        "            iou = 1.0  # Perfect match if both are empty\n",
        "            dice = 1.0\n",
        "\n",
        "        slice_iou.append(iou)\n",
        "        slice_dice.append(dice)\n",
        "\n",
        "    avg_iou = np.mean(slice_iou)\n",
        "    avg_dice = np.mean(slice_dice)\n",
        "\n",
        "    fig.suptitle(f'Slice IoU: {avg_iou:.4f} | Slice Dice: {avg_dice:.4f}',\n",
        "                 fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # plt.show() # This line is removed\n",
        "\n",
        "# Create interactive slider\n",
        "interact(visualize_slice,\n",
        "         slice_idx=IntSlider(min=0, max=num_slices-1, step=1, value=num_slices//2,\n",
        "                            description='Slice Index', style={'description_width': 'initial'}),\n",
        "         ct_data=fixed(ct_data_normalized),\n",
        "         gt_mask=fixed(mask_data),\n",
        "         pred_mask=fixed(predicted_masks),\n",
        "         cmap=fixed(cmap),\n",
        "         norm=fixed(norm))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}